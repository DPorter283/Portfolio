{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import nltk.classify\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "import yellowbrick\n",
    "import os\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.corpus import sentence_polarity\n",
    "from nltk.collocations import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir('C://Users//Damon//OneDrive//NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Sarcasm Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x2364683f438>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(data = df, kind = 'count', x='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>But they'll have all those reviews!</td>\n",
       "      <td>RoguishPoppet</td>\n",
       "      <td>ProductTesting</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>11/1/2016 2:04</td>\n",
       "      <td>The dumb thing is, they are risking their sell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>wow it is totally unreasonable to assume that ...</td>\n",
       "      <td>pb2crazy</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>11/1/2016 2:42</td>\n",
       "      <td>Clinton campaign accuses FBI of 'blatant doubl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Ho ho ho... But Melania said that there is no ...</td>\n",
       "      <td>pb2crazy</td>\n",
       "      <td>politics</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>10/18/2016 16:20</td>\n",
       "      <td>Anyone else think that it was interesting the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't wait until @potus starts a twitter war...</td>\n",
       "      <td>kitduncan</td>\n",
       "      <td>politics</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>11/1/2016 3:22</td>\n",
       "      <td>Here's what happens when Obama gives up his Tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>gotta love the teachers who give exams on the ...</td>\n",
       "      <td>DEP61</td>\n",
       "      <td>CFBOffTopic</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>11/1/2016 3:30</td>\n",
       "      <td>Monday night Drinking thread Brought to You by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Hoiberg said \"great players\", not Dwight</td>\n",
       "      <td>deezee72</td>\n",
       "      <td>nba</td>\n",
       "      <td>29</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>11/1/2016 3:45</td>\n",
       "      <td>except the 2012 lakers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh, I never realized it was so easy, why had I...</td>\n",
       "      <td>SatanicBeaver</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>11/1/2016 3:51</td>\n",
       "      <td>A little self lovin can turn that 99 into a 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Exactly, no reason whatsoever.</td>\n",
       "      <td>Bifi323</td>\n",
       "      <td>oddlysatisfying</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>11/1/2016 9:59</td>\n",
       "      <td>To make predators think they're poisonous?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Hank would have a great time in the Legends of...</td>\n",
       "      <td>CIearMind</td>\n",
       "      <td>supergirlTV</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>11/1/2016 10:23</td>\n",
       "      <td>Pretty inconvenient if they landed anywhere pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>I remember reading all of those stories in the...</td>\n",
       "      <td>MiggidyMacDewi</td>\n",
       "      <td>videos</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>11/1/2016 12:03</td>\n",
       "      <td>You as everyone else are MISSING THE ENTIRE PO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  label                                            comment  \\\n",
       "0   1      1                But they'll have all those reviews!   \n",
       "1   2      1  wow it is totally unreasonable to assume that ...   \n",
       "2   3      1  Ho ho ho... But Melania said that there is no ...   \n",
       "3   4      1  I can't wait until @potus starts a twitter war...   \n",
       "4   5      1  gotta love the teachers who give exams on the ...   \n",
       "5   6      1           Hoiberg said \"great players\", not Dwight   \n",
       "6   7      1  Oh, I never realized it was so easy, why had I...   \n",
       "7   8      1                     Exactly, no reason whatsoever.   \n",
       "8   9      1  Hank would have a great time in the Legends of...   \n",
       "9  10      1  I remember reading all of those stories in the...   \n",
       "\n",
       "           author        subreddit  score  ups  downs     date  \\\n",
       "0   RoguishPoppet   ProductTesting      0   -1     -1  2016-11   \n",
       "1        pb2crazy         politics      2   -1     -1  2016-11   \n",
       "2        pb2crazy         politics      8   -1     -1  2016-10   \n",
       "3       kitduncan         politics      3   -1     -1  2016-11   \n",
       "4           DEP61      CFBOffTopic      3   -1     -1  2016-11   \n",
       "5        deezee72              nba     29   -1     -1  2016-11   \n",
       "6   SatanicBeaver        AskReddit      1   -1     -1  2016-11   \n",
       "7         Bifi323  oddlysatisfying      1   -1     -1  2016-11   \n",
       "8       CIearMind      supergirlTV      3   -1     -1  2016-11   \n",
       "9  MiggidyMacDewi           videos      1   -1     -1  2016-11   \n",
       "\n",
       "        created_utc                                     parent_comment  \n",
       "0    11/1/2016 2:04  The dumb thing is, they are risking their sell...  \n",
       "1    11/1/2016 2:42  Clinton campaign accuses FBI of 'blatant doubl...  \n",
       "2  10/18/2016 16:20  Anyone else think that it was interesting the ...  \n",
       "3    11/1/2016 3:22  Here's what happens when Obama gives up his Tw...  \n",
       "4    11/1/2016 3:30  Monday night Drinking thread Brought to You by...  \n",
       "5    11/1/2016 3:45                             except the 2012 lakers  \n",
       "6    11/1/2016 3:51  A little self lovin can turn that 99 into a 10...  \n",
       "7    11/1/2016 9:59         To make predators think they're poisonous?  \n",
       "8   11/1/2016 10:23  Pretty inconvenient if they landed anywhere pr...  \n",
       "9   11/1/2016 12:03  You as everyone else are MISSING THE ENTIRE PO...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 3 blocks convert the dataframe to a corupus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateCorpusFromDataFrame(corpusfolder,df):\n",
    "    for index, r in df.iterrows():\n",
    "        id=r['ID']\n",
    "        body=r['comment']\n",
    "        category=r['label']\n",
    "        fname=str(category)+'_'+str(id)+'.txt'\n",
    "        corpusfile=open(corpusfolder+'/'+fname,'a')\n",
    "        corpusfile.write(str(body))\n",
    "        corpusfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please create a new path that points to a local directory on your machine. While the corpus contains 20000 records, it is only 6mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CreateCorpusFromDataFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-0870107dd3a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCreateCorpusFromDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C://Users//Damon//OneDrive//NLP//Corpus'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'CreateCorpusFromDataFrame' is not defined"
     ]
    }
   ],
   "source": [
    "CreateCorpusFromDataFrame('C://Users//Damon//OneDrive//NLP//Corpus',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "my_corpus=CategorizedPlaintextCorpusReader('C://Users//Damon//OneDrive//NLP//Corpus',\n",
    "r'.*', cat_pattern=r'(.*)_.*') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus.categories() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(sent, cat) for cat in my_corpus.categories() \n",
    "    for sent in my_corpus.sents(categories=cat)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26529\n",
      "28542\n"
     ]
    }
   ],
   "source": [
    "sarcasticSents = my_corpus.sents(categories= '1')\n",
    "print(len(sarcasticSents))\n",
    "nonsarcasticSents = my_corpus.sents(categories= '0')\n",
    "print(len(nonsarcasticSents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list = [word.lower() for (sent,cat) in documents for word in sent]\n",
    "all_words  = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(1500)\n",
    "word_features = [word for (word, freq) in word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', \"'\", '.', ',', 'a', 'to', 'you', 'i', 'and', 'it', 'is', 'that', 's', 'of', 't', '?', 'in', 'for', '!', '-', 'this', 'they', 'be', '\"', 'have', 'on', 'are', 'but', 'he', 'just', 'not', 'with', 'was', '...', 'so', 'like', 'can', 'all', 'if', 'we', 'as', 'at', 'your', 'because', 'my', 'would', 'trump', 're', 'people', 'what']\n"
     ]
    }
   ],
   "source": [
    "print(word_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55071\n"
     ]
    }
   ],
   "source": [
    "print(len(featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.626875\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[49600:], featuresets[:49600]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  V_kids = True                1 : 0      =     14.1 : 1.0\n",
      "                  V_jobs = True                1 : 0      =      9.1 : 1.0\n",
      "               V_dropped = True                1 : 0      =      8.3 : 1.0\n",
      "               V_totally = True                1 : 0      =      8.1 : 1.0\n",
      "             V_reference = True                0 : 1      =      7.7 : 1.0\n",
      "                 V_close = True                0 : 1      =      7.7 : 1.0\n",
      "               V_picture = True                1 : 0      =      7.6 : 1.0\n",
      "                   V_ass = True                0 : 1      =      6.4 : 1.0\n",
      "              V_possible = True                0 : 1      =      6.4 : 1.0\n",
      "               V_usually = True                0 : 1      =      6.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(num_folds, featuresets):\n",
    "    subset_size = len(featuresets)//num_folds\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[i*subset_size:][:subset_size]\n",
    "        train_this_round = featuresets[:i*subset_size]+featuresets[(i+1)*subset_size:]\n",
    "            # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "# evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier, test_this_round)\n",
    "        print (i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "    # find mean accuracy over all rounds\n",
    "    print ('mean accuracy', sum(accuracy_list) / num_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6518494307348696\n",
      "1 0.6607833523996296\n",
      "2 0.658386446587133\n",
      "mean accuracy 0.6570064099072107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18357.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(3, featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    goldlist.append(label)\n",
    "    predictedlist.append(classifier.classify(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |      0      1 |\n",
      "--+---------------+\n",
      "0 | <33.4%> 18.5% |\n",
      "1 |  17.7% <30.4%>|\n",
      "--+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "0 \t      0.644      0.653      0.649\n",
      "1 \t      0.632      0.622      0.627\n"
     ]
    }
   ],
   "source": [
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Stopwords and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove stop words\n",
    "# Add Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "0## Remove stop words\n",
    "stopWords = set(stopwords.words('english'))\n",
    "all_words_list = [lemmatizer.lemmatize(word.lower()) for (sent,cat) in documents for word in sent if word not in stopWords]\n",
    "all_words  = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(1500)\n",
    "word_features = [word for (word, freq) in word_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", '.', ',', 'i', '?', '!', '-', '\"', '...', 'like', 'get', 'trump', 'would', 'people', 'one', '*', 'know', 'make', 'think', 'right', 'donald', 'game', 'good', 'time', 'dump', 'tronald', '/', ':', 'really', 'need', 'sure', 'well', 'thing', 'mean', 'see', 'want', 'could', 'guy', 'yeah', 'go', 'year', 'you', 'way', 'much', 'the', 'better', 'even', 'man', 'look', 'never']\n"
     ]
    }
   ],
   "source": [
    "print(word_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondfeaturesets = [(document_features(d,word_features), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5952016129032258\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = secondfeaturesets[49600:], secondfeaturesets[:49600]\n",
    "secondClassifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(secondClassifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   V_men = True                1 : 0      =      8.7 : 1.0\n",
      "                V_number = True                0 : 1      =      7.9 : 1.0\n",
      "                 V_haven = True                0 : 1      =      7.9 : 1.0\n",
      "                 V_water = True                0 : 1      =      7.9 : 1.0\n",
      "               V_totally = True                1 : 0      =      7.3 : 1.0\n",
      "                V_friend = True                0 : 1      =      7.3 : 1.0\n",
      "                 V_these = True                1 : 0      =      6.6 : 1.0\n",
      "                  V_save = True                1 : 0      =      6.6 : 1.0\n",
      "                  V_lazy = True                1 : 0      =      6.6 : 1.0\n",
      "             V_obviously = True                1 : 0      =      6.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "secondClassifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6233044615133192\n",
      "1 0.626899820232064\n",
      "2 0.6286430244593343\n",
      "mean accuracy 0.6262824354015725\n"
     ]
    }
   ],
   "source": [
    "cross_validation(3, secondfeaturesets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |      0      1 |\n",
      "--+---------------+\n",
      "0 | <27.3%> 24.6% |\n",
      "1 |  15.9% <32.2%>|\n",
      "--+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    goldlist.append(label)\n",
    "    predictedlist.append(secondClassifier.classify(features))\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "1 \t      0.669      0.567      0.614\n",
      "0 \t      0.527      0.632      0.575\n"
     ]
    }
   ],
   "source": [
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams & Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictWord = set(nltk.corpus.words.words())\n",
    "all_words_list = [word.lower() for (sent,cat) in documents for word in sent if word in dictWord]\n",
    "all_words  = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(1500)\n",
    "word_features = [word for (word, freq) in word_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "triFinder = TrigramCollocationFinder.from_words(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_features = finder.nbest(bigram_measures.pmi,500)\n",
    "trigram_features = triFinder.nbest(trigram_measures.pmi,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_document_features(document,word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bifeaturesets = [(bigram_document_features(d,word_features, bigram_features), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5172357723577236\n"
     ]
    }
   ],
   "source": [
    "bitrain_set, bitest_set = bifeaturesets[49200:], bifeaturesets[:49200]\n",
    "biclassifier = nltk.NaiveBayesClassifier.train(bitest_set)\n",
    "print (nltk.classify.accuracy(biclassifier, bitest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "   B_morbid_apprehension = False               1 : 0      =      1.0 : 1.0\n",
      "   B_commissioner_gordon = False               1 : 0      =      1.0 : 1.0\n",
      "            B_spelt_duck = False               1 : 0      =      1.0 : 1.0\n",
      "       B_tread_patterned = False               1 : 0      =      1.0 : 1.0\n",
      "         B_horrific_pose = False               1 : 0      =      1.0 : 1.0\n",
      "     B_childish_childish = False               1 : 0      =      1.0 : 1.0\n",
      "B_universally_unconstitutional = False               1 : 0      =      1.0 : 1.0\n",
      "      B_strategic_bomber = False               1 : 0      =      1.0 : 1.0\n",
      "           B_vista_brick = False               1 : 0      =      1.0 : 1.0\n",
      " B_typically_solipsistic = False               1 : 0      =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "biclassifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6239036879664434\n",
      "1 0.6234678869096257\n",
      "2 0.6293512011766629\n",
      "mean accuracy 0.625574258684244\n"
     ]
    }
   ],
   "source": [
    "cross_validation(3, bifeaturesets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |      0      1 |\n",
      "--+---------------+\n",
      "0 | <27.5%> 24.2% |\n",
      "1 |  15.8% <32.4%>|\n",
      "--+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    goldlist.append(label)\n",
    "    predictedlist.append(biclassifier.classify(features))\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "0 \t      0.532      0.635      0.579\n",
      "1 \t      0.672      0.572      0.618\n"
     ]
    }
   ],
   "source": [
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_document_features(document,word_features, bigram_features, trigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    document_trigram = nltk.trigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    for trigram in trigram_features:\n",
    "         features['T_{}_{}_{}'.format(trigram[0], trigram[1], trigram[2])] = (trigram in document_trigram)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trifeaturesets = [(trigram_document_features(d, word_features, bigram_features,trigram_features), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.599390243902439\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = trifeaturesets[49200:], trifeaturesets[:49200]\n",
    "triclassifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(triclassifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             V_obviously = True                1 : 0      =      8.8 : 1.0\n",
      "                   V_may = True                0 : 1      =      8.1 : 1.0\n",
      "                   V_per = True                1 : 0      =      7.8 : 1.0\n",
      "               V_similar = True                0 : 1      =      6.9 : 1.0\n",
      "                 V_small = True                0 : 1      =      6.6 : 1.0\n",
      "                   V_ass = True                0 : 1      =      6.3 : 1.0\n",
      "                  V_near = True                0 : 1      =      6.3 : 1.0\n",
      "                 V_night = True                0 : 1      =      6.3 : 1.0\n",
      "              V_campaign = True                0 : 1      =      6.3 : 1.0\n",
      "                 V_under = True                0 : 1      =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "triclassifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6233044615133192\n",
      "1 0.626899820232064\n",
      "2 0.6286430244593343\n",
      "mean accuracy 0.6262824354015725\n"
     ]
    }
   ],
   "source": [
    "cross_validation(3, trifeaturesets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |      0      1 |\n",
      "--+---------------+\n",
      "0 | <27.2%> 24.7% |\n",
      "1 |  15.7% <32.4%>|\n",
      "--+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    goldlist.append(label)\n",
    "    predictedlist.append(triclassifier.classify(features))\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "1 \t      0.673      0.567      0.616\n",
      "0 \t      0.524      0.634      0.574\n"
     ]
    }
   ],
   "source": [
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "newClassFeatureSets = featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = newClassFeatureSets[49200:], newClassFeatureSets[:49200]\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set, binary=True, entropy_cutoff=0.8, depth_cutoff=5, support_cutoff=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5905284552845529\n",
      "V_!=False? ............................................ 0\n",
      "  V_because=False? .................................... 0\n",
      "    V_obviously=False? ................................ 0\n",
      "      V_all=False? .................................... 0\n",
      "      else: ........................................... 1\n",
      "    else: ............................................. 1\n",
      "  else: ............................................... 1\n",
      "    V_it=False? ....................................... 1\n",
      "      V_know=False? ................................... 1\n",
      "      else: ........................................... 0\n",
      "    else: ............................................. 0\n",
      "      V_.=False? ...................................... 1\n",
      "      else: ........................................... 0\n",
      "else: ................................................. 0\n",
      "  V_nice=False? ....................................... 0\n",
      "    V_too=False? ...................................... 0\n",
      "      V_feels=False? .................................. 0\n",
      "      else: ........................................... 0\n",
      "    else: ............................................. 0\n",
      "      V_people=False? ................................. 0\n",
      "      else: ........................................... 1\n",
      "  else: ............................................... 0\n",
      "    V_get=False? ...................................... 0\n",
      "    else: ............................................. 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy: 0.6452439024390244\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "MultinomialNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MultinomialNB_classifier.train(train_set)\n",
    "print(\"MultinomialNB accuracy:\",nltk.classify.accuracy(MultinomialNB_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB accuracy: 0.6271138211382113\n"
     ]
    }
   ],
   "source": [
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(train_set)\n",
    "print(\"BernoulliNB accuracy:\",nltk.classify.accuracy(BernoulliNB_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLpath  = \"C://Users//Damon//OneDrive//NLP//subjclueslen1-HLTEMNLP05.tff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSubjectivity(path):\n",
    "    flexicon = open(path, 'r')\n",
    "    # initialize an empty dictionary\n",
    "    sldict = { }\n",
    "    for line in flexicon:\n",
    "        fields = line.split()   # default is to split on whitespace\n",
    "        # split each field on the '=' and keep the second part as the value\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        # put a dictionary entry with the word as the keyword\n",
    "        #     and a list of the other values\n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weaksubj', 'noun', False, 'negative']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SL = readSubjectivity(SLpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SL_features(document, SL):\n",
    "    SL = readSubjectivity(SLpath)\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "        # count variables for the 4 classes of subjectivity\n",
    "        weakPos = 0\n",
    "        strongPos = 0\n",
    "        weakNeg = 0\n",
    "        strongNeg = 0\n",
    "        for word in document_words:\n",
    "            if word in SL:\n",
    "                strength, posTag, isStemmed, polarity = SL[word]\n",
    "                if strength == 'weaksubj' and polarity == 'positive':\n",
    "                    weakPos += 1\n",
    "                if strength == 'strongsubj' and polarity == 'positive':\n",
    "                    strongPos += 1\n",
    "                if strength == 'weaksubj' and polarity == 'negative':\n",
    "                    weakNeg += 1\n",
    "                if strength == 'strongsubj' and polarity == 'negative':\n",
    "                    strongNeg += 1\n",
    "        features['positivecount'] = weakPos + (2 * strongPos)\n",
    "        features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_featuresets = [(SL_features(d, SL), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55071\n"
     ]
    }
   ],
   "source": [
    "print(len(SL_featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6246544715447154\n"
     ]
    }
   ],
   "source": [
    "SLtrain_set, SLtest_set = SL_featuresets[49200:], SL_featuresets[:49200]\n",
    "SLclassifier = nltk.NaiveBayesClassifier.train(SLtrain_set)\n",
    "print (nltk.classify.accuracy(SLclassifier, SLtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6614915291169581\n",
      "1 0.663125783080024\n",
      "2 0.6620907555700822\n",
      "mean accuracy 0.6622360225890214\n"
     ]
    }
   ],
   "source": [
    "cross_validation(3, SL_featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            contains().) = True                0 : 1      =     10.5 : 1.0\n",
      "       contains(dropped) = True                1 : 0      =      9.3 : 1.0\n",
      "     contains(obviously) = True                1 : 0      =      8.8 : 1.0\n",
      "         contains(women) = True                1 : 0      =      8.2 : 1.0\n",
      "           contains(may) = True                0 : 1      =      8.1 : 1.0\n",
      "           contains(per) = True                1 : 0      =      7.8 : 1.0\n",
      "            contains(:)) = True                0 : 1      =      6.9 : 1.0\n",
      "       contains(similar) = True                0 : 1      =      6.9 : 1.0\n",
      "         contains(small) = True                0 : 1      =      6.6 : 1.0\n",
      "          contains(nazi) = True                1 : 0      =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "SLclassifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |      0      1 |\n",
      "--+---------------+\n",
      "0 | <30.6%> 21.1% |\n",
      "1 |  16.4% <31.8%>|\n",
      "--+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in SLtest_set:\n",
    "    goldlist.append(label)\n",
    "    predictedlist.append(SLclassifier.classify(features))\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "0 \t      0.592      0.651      0.620\n",
      "1 \t      0.660      0.601      0.629\n"
     ]
    }
   ],
   "source": [
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.DecisionTreeClassifier.train(SLtrain_set, binary=True, entropy_cutoff=0.8, depth_cutoff=5, support_cutoff=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5908943089430895\n",
      "contains(!)=False? .................................... 0\n",
      "  contains(because)=False? ............................ 0\n",
      "    contains(obviously)=False? ........................ 0\n",
      "      contains(all)=False? ............................ 0\n",
      "      else: ........................................... 1\n",
      "    else: ............................................. 1\n",
      "  else: ............................................... 1\n",
      "    contains(it)=False? ............................... 1\n",
      "      contains(know)=False? ........................... 1\n",
      "      else: ........................................... 0\n",
      "    else: ............................................. 0\n",
      "      contains(.)=False? .............................. 1\n",
      "      else: ........................................... 0\n",
      "else: ................................................. 0\n",
      "  contains(nice)=False? ............................... 0\n",
      "    contains(every)=False? ............................ 0\n",
      "      contains(open)=False? ........................... 1\n",
      "      else: ........................................... 0\n",
      "    else: ............................................. 1\n",
      "      contains(is)=False? ............................. 0\n",
      "      else: ........................................... 1\n",
      "  else: ............................................... 0\n",
      "    contains(logic)=False? ............................ 0\n",
      "    else: ............................................. 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, SLtest_set))\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy: 0.6466463414634146\n"
     ]
    }
   ],
   "source": [
    "MultinomialNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MultinomialNB_classifier.train(SLtrain_set)\n",
    "print(\"MultinomialNB accuracy:\",nltk.classify.accuracy(MultinomialNB_classifier, SLtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB accuracy: 0.6283536585365853\n"
     ]
    }
   ],
   "source": [
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(SLtrain_set)\n",
    "print(\"BernoulliNB accuracy:\",nltk.classify.accuracy(BernoulliNB_classifier, SLtest_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
